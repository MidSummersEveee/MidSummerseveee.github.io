---
layout: post
title: PCA in a Nutshell 主成因分析引论
excerpt: "从应用情境到数学原理梳理诠释PCA的精要"
modified: 24/08/2018, 19:45:5
tags: [PCA]
comments: true
category: blog
---

## 1. 主成因分析的场景


Principal Component Analysis (PCA) 主成因分析的应用场景是很简单的，其需求源于我们对数据的观测和思考。
**数据的基本形态是“样本” Sample 和“特征” Feature**，先看一例：

|| Mouse1 |Mouse2 |Mouse3|Mouse4|Mouse5|Mouse6|
|--------|--------|--------|--|--|--|--|
| **Gen1**  |   10|11|8|3|2|1|
| **Gen2**  |    6|4|5|3|2.8|1|


在上表中，行为某种基因的表达指数，列为一个实验鼠的样本.
如果我们把上图plot一下，就会发现这些样本中有明显的cluster现象。
<img src="https://MidSummerseveee.github.io/images/PCA1.png" width="380" height="100" />

注意横轴为Gene1，纵轴为Gene2。如果还有Gene3的话，我们还是可以用类似的方法plot数据，此时样本点就在三维空间中分布了。如果有多于或等于四维的情况，图就不太好展现了，此时我们就希望有一种能够降维的方法，让我们还能在三维或二维视界直观察看到cluster现象。

主成因分析法正好派上用场，**此外，PCA还能告诉我们:**
- **哪个基因在cluster样本方面最具价值**。(如：Gene2是导致样本cluser，即分化的最主要原因)
- **最后的二维视图有多么准确**。（Gene1和Gene2在最后的二维视图里贡献度如何）

## 2. PCA breakdown
首先，**我们获取一下所有样本的均值，由此确定了中心点m**，接下来只需要把注意力放在散点图上就好，

接下来，**我们做一个滑动操作，使得坐标轴的中心来到中心点m的位置**（或者相对地移动所有样本点）。
注意这个滑动操作是直线位移，因此并不会改变所有样本点之间的相对分布。

数据已经就位，**现在我们需要找一条通过新原点的直线，且使得这条直线最"fit"所有样本点**。
最fit这种说法是很模糊的，怎样算是fit呢？

这里的思路和线性回归是一样的。
<img src="https://MidSummerseveee.github.io/images/PCA2.png" width="380" height="100" />

## 3. Math behind PCA
本节做一些对PCA数学原理的基本讨论，需要一定的线性代数知识。
 - **数据矩阵**
在实际的应用情境中，加载得到的数据矩阵一般是**NxD**的，亦即每列为一个维度，每行为一个样本。
在经典的线性代数语境中，我们则喜欢用**DxN**的形式，即矩阵的每个列向量为一样本，首节中的基因与鼠的例子就是这个形式。

**从数学角度看，PCA的目的很纯粹，给定一数据矩阵X，PCA能够确定一个特定的线性变换Q，使得变换结果Z的协方差(Coviriance)矩阵为对角矩阵。**

Coviriance矩阵是对角矩阵表明，每个新的维度之间没有任何协变性，此时信息的冗余程度大大减小（某些维度之间存在协变关系，则说明可在一定程度上由一个维度的值推测另一个，是典型的信息redundancy）。

<img src="https://MidSummerseveee.github.io/images/PCA3.png" width="380" height="100" />

上图是一个完整的数学背景展示，以下是其中三个关键部分的解读：

 1. **基变换解决维度消减**
 记住，我们的目的是对原来的超高维数据进行处理，去除冗余信息并缩减维度，得到一个维度较少，且各维度无相关性（各维度间协方差为0）的表达形式。前者叫做**Dimensionality reduction**（维度消减），后者称之为**De-correlation**（去相关）。
看上去要同时完成两个任务不太轻松，其实只需通过一个合适的基变换就可以达成。
在基变换时，我们首先选定新基，以每个新基的成员向量作为列向量，可以得到Change of Bases Matrix 基变换矩阵，图中简记为C。
基变换的两种等价形式如下，[***α***]表示α with the respect of C，即*α*在新基C下的表示。

        C [α] = α
        C ^(-1) α = [α]
	显然，只需使新基的维度少于原始数据的维度，变换后维度自然消减。
	需要重点解决的是去相关问题。
 
 

 2. **实际与理论结合**
 前文已经讲到，在实际处理的过程中，我们加载的数据矩阵X通常是NxD的，而我们熟悉的基变换一般是对各列向量而言的变换，此时需要将原基变换稍做处理，为方便起见，我们先按照经典线性代数的概念经行基变换，即对习惯上的X^T进行变换，在得到结果后，再转置一次，此时使等式右侧变为：数据矩阵X直接右乘待求矩阵Q。即`(C^(-1)X^T)^T = XQ`
右侧，我们之所以把Q定为待求矩阵，是因为在实际中一般我们手上拿到的是X，直接对其进行Q变换是很方便的。
左侧，是我们熟悉的经典线性代数基变换表达。
**求解该等式后我们发现，只需使得C代表的基为标准正交基，即可得到C=Q的结论。**

 3. **基变换解决去相关**
 如何选取无关的新基呢？上文已经谈到，只要使得变换后的数据矩阵Z对应的协方差矩阵Σz为一对角矩阵即可。
 我们先从Σz开始进行一些推导，看一看Σz和Σx的关系
 如同原理图中显示的，只要我们把待求矩阵Q选为原协方差矩阵Σx对角化时采用的标准花特征向量矩阵，问题自然得到求解。
 此时为经典表达X^T选取的变换基和待求矩阵Q都恰好为该特征向量标准正交基。
